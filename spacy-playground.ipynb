{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp_de('Ich habe ein sch√∂nes Auto')\n",
    "# doc = nlp_de('Die Firma Xing hat dich zum data insights Event eingeladen')\n",
    "doc = nlp_de(\"Er arebitet im Bereich Softwareentwicklung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"de\" id=\"97c9020bd65c4a178651c6597c0ed7f2-0\" class=\"displacy\" width=\"925\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Er</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">arebitet</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">im</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Bereich</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Softwareentwicklung</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97c9020bd65c4a178651c6597c0ed7f2-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97c9020bd65c4a178651c6597c0ed7f2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">sb</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97c9020bd65c4a178651c6597c0ed7f2-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97c9020bd65c4a178651c6597c0ed7f2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97c9020bd65c4a178651c6597c0ed7f2-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97c9020bd65c4a178651c6597c0ed7f2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97c9020bd65c4a178651c6597c0ed7f2-0-3\" stroke-width=\"2px\" d=\"M595,89.5 C595,2.0 750.0,2.0 750.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97c9020bd65c4a178651c6597c0ed7f2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,91.5 L758.0,79.5 742.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Die Firma \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Xing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " hat dich eingeladen zu dem \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    data insights Event\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_en(\"The pig ate all the food.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, pig, ate, all, the, food, .]\n"
     ]
    }
   ],
   "source": [
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, 'd, rather, not, eat, that, !, !, !, ,, it, 's, rat, poison, ., \n",
      " , and, I, prefer, mango, juice]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en(\"I'd rather not eat that!!!, it's rat poison.\\n and I prefer mango juice\")\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I'd rather not eat that!!!, , it's rat poison., \n",
      " and I prefer mango juice]\n"
     ]
    }
   ],
   "source": [
    "# sentences\n",
    "print([token for token in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "went go\n",
      "there there\n",
      "for for\n",
      "working work\n",
      "and and\n",
      "worked work\n",
      "for for\n",
      "3 3\n",
      "years year\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I went there for working and worked for 3 years.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "am be\n",
      "flying fly\n",
      "to to\n",
      "Angeltown Los Angeles\n"
     ]
    }
   ],
   "source": [
    "# adding custom lemmatization. useful for nicknames\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Angeltown\"}]], {\"LEMMA\": \"Los Angeles\"})\n",
    "doc = nlp(\"I am flying to Angeltown\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load('de_core_news_lg')\n",
    "doc = nlp_de(\"\"\"\n",
    "Hallo Herr Alfermann,\n",
    "im Rahmen unserer Personalrecherche bin ich auf Ihr Profil gesto√üen. Ich bin bei Robert Half f√ºr das Interne Recruiting in Hamburg und Berlin zust√§ndig. Robert Half ist ein internationales Unternehmen und einer der ersten und gr√∂√üten Personaldienstleitungsunternehmen weltweit. Aufgrund unseres deutschlandweiten Expansionskurses sind wir auf der Suche nach Consultants und Resource Manager f√ºr Freelancer in Hamburg.\n",
    "Wenn Sie offen f√ºr einen Wechsel sind und sich mit mir √ºber einen Einstieg bei Robert Half austauschen m√∂chten, freue ich mich √ºber Ihren Lebenslauf.\n",
    "Gerne k√∂nnen wir auch einen Termin f√ºr ein telefonische Gespr√§ch vereinbaren.\n",
    "Ich freue mich auf Ihr Feedback!\n",
    "Viele Gr√º√üe\n",
    "Ragna Paulsen\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(doc.sents) # sentences \n",
    "# list(doc.ents) # entities\n",
    "# list(doc.noun_chunks) # noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token.is_alpha\n",
    "# token.is_currency\n",
    "# token.is_punct\n",
    "# token.like_email\n",
    "# token.like_url\n",
    "# token.like_num\n",
    "# for token in doc:\n",
    "# \tif token.is_oov:\n",
    "# \t\tprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flowers', 'NNS', 'noun, plural')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"I saw flowers.\")\n",
    "token = doc[2]\n",
    "token.text, token.tag_, spacy.explain(token.tag_)\n",
    "('flowers', 'NNS', 'noun, plural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Alicia, pos: PROPN, tag: NNP\n",
      "proper noun\n",
      "noun, proper singular\n",
      "token: and, pos: CCONJ, tag: CC\n",
      "coordinating conjunction\n",
      "conjunction, coordinating\n",
      "token: me, pos: PRON, tag: PRP\n",
      "pronoun\n",
      "pronoun, personal\n",
      "token: went, pos: VERB, tag: VBD\n",
      "verb\n",
      "verb, past tense\n",
      "token: to, pos: ADP, tag: IN\n",
      "adposition\n",
      "conjunction, subordinating or preposition\n",
      "token: the, pos: DET, tag: DT\n",
      "determiner\n",
      "determiner\n",
      "token: school, pos: NOUN, tag: NN\n",
      "noun\n",
      "noun, singular or mass\n",
      "token: by, pos: ADP, tag: IN\n",
      "adposition\n",
      "conjunction, subordinating or preposition\n",
      "token: bus, pos: NOUN, tag: NN\n",
      "noun\n",
      "noun, singular or mass\n",
      "token: ., pos: PUNCT, tag: .\n",
      "punctuation\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Alicia and me went to the school by bus.\")\n",
    "for token in doc:\n",
    "\tprint(f\"token: {token.text}, pos: {token.pos_}, tag: {token.tag_}\")\n",
    "\tprint(spacy.explain(token.pos_))\n",
    "\tprint(spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cat', 'dobj', 'direct object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat is a direct object. The word that the verb applies on.\n",
    "doc = nlp(\"I own a ginger cat.\")\n",
    "token = doc[4]\n",
    "token.text, token.dep_, spacy.explain(token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP nsubj counted\n",
      "counted VBD ROOT counted\n",
      "white JJ amod sheep\n",
      "sheep NN dobj counted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7e928d8d5bb04304b7c898bd5f8c24bb-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">counted</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">white</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sheep</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7e928d8d5bb04304b7c898bd5f8c24bb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"I counted white sheep\")\n",
    "\n",
    "for token in doc:\n",
    "      print(token.text, token.tag_, token.dep_, token.head)\n",
    "\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ORG', 'Companies, agencies, institutions, etc.')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")\n",
    "doc = nlp(\"He worked for XING.\")\n",
    "token = doc[3]\n",
    "token.ent_type_, spacy.explain(token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert PERSON People, including fictional\n",
      "Einstein PERSON People, including fictional\n",
      "was  None\n",
      "born  None\n",
      "in  None\n",
      "Ulm GPE Countries, cities, states\n",
      "on  None\n",
      "1879 DATE Absolute or relative dates or periods\n",
      ".  None\n",
      "He  None\n",
      "studied  None\n",
      "electronical  None\n",
      "engineering  None\n",
      "at  None\n",
      "ETH ORG Companies, agencies, institutions, etc.\n",
      "Zurich ORG Companies, agencies, institutions, etc.\n",
      ".  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilo.ramirez/Library/Caches/pypoetry/virtualenvs/spacy-playground-YteZBk1j-py3.8/lib/python3.8/site-packages/spacy/glossary.py:19: UserWarning: [W118] Term '' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Albert Einstein was born in Ulm on 1879. He studied electronical engineering at ETH Zurich.\")\n",
    "doc.ents\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.ent_type_, spacy.explain(token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load recruiter messages and perform an entity counting like in the code below.\n",
    "# check chapter 3 of the book Mastering Spacy.\n",
    "# from collections import Counter\n",
    "# labels = [ent.label_ for ent in doc.ents]\n",
    "# Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Good morning, I want to reserve a ticket.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"},{\"IS_PUNCT\": True}]\n",
    "\n",
    "matcher.add(\"morningGreeting\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "     m_span = doc[start:end]  \n",
    "     print(start, end, m_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 hello,\n",
      "1 4 hello hello,\n",
      "0 4 Hello hello hello,\n"
     ]
    }
   ],
   "source": [
    "# A more complex matching example\n",
    "\n",
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "doc3 = nlp(\"How are you?\")\n",
    "\n",
    "# check both * and + to see the difference\n",
    "pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]}, \"OP\":\"+\"}, {\"IS_PUNCT\": True}]\n",
    "# pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]}, \"OP\":\"*\"}, {\"IS_PUNCT\": True}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"greetings\",  [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "     print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 name is Alice\n",
      "6 9 name was Elliot\n",
      "9 10 .\n"
     ]
    }
   ],
   "source": [
    "# wildcards\n",
    "doc = nlp(\"My name is Alice and his name was Elliot.\")\n",
    "pattern = [{\"LOWER\": \"name\"},{\"LEMMA\": \"be\"},{}]\n",
    "matcher.add(\"pickName\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "     print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Ich bin bei\n",
      "6 9 Er war in\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp_de(\"Ich bin bei der Firma X. Er war in K√∂ln\")\n",
    "\n",
    "# {} wildcard token means any token\n",
    "pattern = [{\"POS\": \"PRON\"},{\"LEMMA\": \"sein\"},{}]\n",
    "\n",
    "matcher = Matcher(nlp_de.vocab)\n",
    "matcher.add(\"pickName\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "     print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 I travelled\n",
      "0 2 She traveled\n"
     ]
    }
   ],
   "source": [
    "# working wirh regex\n",
    "doc1 = nlp(\"I travelled by bus.\")\n",
    "doc2 = nlp(\"She traveled by bike.\")\n",
    "pattern = [{\"POS\": \"PRON\"},{\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"langVariatons\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "     print(start, end, doc1[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc2):\n",
    "     print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 went\n",
      "6 7 has\n",
      "7 8 been\n",
      "14 15 has\n",
      "15 16 told\n",
      "18 19 wants\n",
      "20 21 visit\n"
     ]
    }
   ],
   "source": [
    "# using regex with POS tags\n",
    "\n",
    "doc = nlp(\"I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.\")\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "matcher.add(\"verbs\",  [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 derivatives\n",
      "6 7 market\n",
      "9 10 asset\n"
     ]
    }
   ],
   "source": [
    "# Phrase matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "terms = [\"Asset\", \"Investment\", \"Derivatives\", \"Demand\",  \"Market\"]\n",
    "\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"financeTerms\", patterns)\n",
    "\n",
    "doc = nlp(\"During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Bill\n",
      "1 2 Gates\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"PERSON\"}]\n",
    "\n",
    "doc = nlp(\"Bill Gates visited Berlin.\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"personEnt\",  [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 Merkel met\n",
      "3 6 Angela Merkel met\n"
     ]
    }
   ],
   "source": [
    "# Matching two conseutive tokens,  one is an entity type and the other is a POS tag\n",
    "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\": \"+\"}, {\"POS\" : \"VERB\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"personEntAction\",  [pattern])\n",
    "doc = nlp(\"Today German chancellor Angela Merkel met with the US president.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017,)\n",
      "(chime, 2017)\n"
     ]
    }
   ],
   "source": [
    "# add custom entities\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"I have an acccount with chime since 2017\")\n",
    "print(doc.ents)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"chime\"}]}]\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(\"I have an acccount with chime since 2017\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Spacy Models and Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 FR76 3000\n",
      "4 7 FR76 3000 6000\n",
      "4 8 FR76 3000 6000 0112\n",
      "4 9 FR76 3000 6000 0112 3456\n",
      "4 10 FR76 3000 6000 0112 3456 7890\n",
      "4 11 FR76 3000 6000 0112 3456 7890 189\n"
     ]
    }
   ],
   "source": [
    "# finding iban numbers\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"My IBAN number is BE71 0961 2345 6769, please send the money there.\")\n",
    "doc1 = nlp(\"My IBAN number is FR76 3000 6000 0112 3456 7890 189, please send the money there.\")\n",
    "\n",
    "pattern = [{\"SHAPE\": \"XXdd\"},{\"TEXT\": {\"REGEX\": \"\\d{1,4}\"}, \"OP\":\"+\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"ibanNum\",  [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 #WeekendShred\n"
     ]
    }
   ],
   "source": [
    "# hash tags matching\n",
    "doc = nlp(\"Start working out now #WeekendShred\")\n",
    "pattern = [{\"TEXT\": \"#\"}, {\"IS_ASCII\": True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"hashTag\",  [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "\tprint(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 üòÄ\n"
     ]
    }
   ],
   "source": [
    "# emojis matching\n",
    "pos_emoji = [\"üòÄ\", \"ü¶ù\", \"\", \"\", \"\", \"\"] \n",
    "neg_emoji = [\"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"posEmoji\", pos_patterns)\n",
    "matcher.add(\"negEmoji\", neg_patterns)\n",
    "\n",
    "doc = nlp(\" I love Zara üòÄ\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Einstein', 'PERSON'), ('Zurich', 'GPE')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Einstein lived in Zurich.\")\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Einstein, 'city': [Zurich], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "person_ents = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "for person_ent in person_ents:\n",
    "\n",
    "\t#We use head of the entity's last token\n",
    "\thead = person_ent[-1].head  \n",
    "\n",
    "\tif head.lemma_ == \"live\":\n",
    "\t\tpreps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "\n",
    "\t\tfor prep in preps:\n",
    "\t\t\tplaces = [token for token in prep.children if token.ent_type_ == \"GPE\"]   \n",
    "\t\t\t\n",
    "\t\t\t# Verb is in past or present tense\n",
    "\t\t\tprint({'person': person_ent, 'city': places,'past': head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors and Semantic Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing text with semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Parsing with spaCy: Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"data/atis_intents.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                                                  1\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what is the arrival time in san francisco for...\n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando\n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n",
      " what flights are available from pittsburgh to baltimore on thursday morning\n",
      " what is the arrival time in san francisco for the 755 am flight leaving washington\n",
      " cheapest airfare from tacoma to orlando\n",
      " round trip fares from pittsburgh to philadelphia under 1000 dollars\n"
     ]
    }
   ],
   "source": [
    "for text in dataset[1].head():\n",
    "\tprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "atis_abbreviation                           147\n",
      "atis_aircraft                                81\n",
      "atis_aircraft#atis_flight#atis_flight_no      1\n",
      "atis_airfare                                423\n",
      "atis_airfare#atis_flight_time                 1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "grouped = dataset.groupby(0).size().head()\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'GPE': 9124, 'DATE': 1474, 'TIME': 994, 'ORG': 428, 'CARDINAL': 317, 'ORDINAL': 218, 'NORP': 74, 'QUANTITY': 44, 'MONEY': 42, 'LOC': 16, 'PRODUCT': 12, 'FAC': 9, 'PERSON': 6, 'EVENT': 1})\n"
     ]
    }
   ],
   "source": [
    "# calculating the frequency of the entities in all documents\n",
    "\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "corpus = open(\"data/atis_utterances.txt\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "all_ent_labels = []\n",
    "\n",
    "for sentence in corpus:\n",
    "\tdoc = nlp(sentence.strip())\n",
    "\tents = doc.ents\n",
    "\tall_ent_labels += [ent.label_ for ent in ents]\n",
    "\n",
    "c = Counter(all_ent_labels)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from denver\n",
      "to boston\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"ADP\"}, {\"ENT_TYPE\": \"GPE\"}]\n",
    "matcher.add(\"prepositionLocation\", [pattern])\n",
    "\n",
    "doc = nlp(\"show me flights from denver to boston on tuesday\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united\n",
      "united airlines\n",
      "airlines\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"ORG\", \"OP\": \"+\"}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"AirlineName\", [pattern])\n",
    "doc = nlp(\"what is the earliest united airlines flight flying from denver\")\n",
    "matches = matcher(doc)\n",
    "for mid,start,end in matches:\n",
    "\tprint(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding abbreviations\n",
    "\n",
    "pattern1 = [{\"TEXT\": {\"REGEX\": \"\\w{1,2}\\d{1,2}\"}}]\n",
    "\n",
    "pattern2 = [{\"SHAPE\": { \"IN\": [\"x\", \"xx\"]}}, {\"SHAPE\": { \"IN\": [\"d\", \"dd\"]}}]\n",
    "\n",
    "pattern3 = [{\"TEXT\": {\"IN\": [\"class\", \"code\", \"abbrev\", \"abbreviation\"]}}, {\"SHAPE\": { \"IN\": [\"x\", \"xx\"]}}]\n",
    "\n",
    "pattern4 =   [{\"POS\": \"NOUN\", \"SHAPE\": { \"IN\": [\"x\", \"xx\"]}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"abbrevEntities\", [pattern1, pattern2, pattern3, pattern4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap 57\n",
      "57\n",
      "abbreviation co\n",
      "co\n",
      "code qo\n",
      "d10\n",
      "code y\n",
      "code f\n",
      "class c\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "'what does restriction ap 57 mean',\n",
    "'what does the abbreviation co mean',\n",
    "'what does fare code qo mean',\n",
    "'what is the abbreviation d10',\n",
    "'what does code y mean',\n",
    "'what does the fare code f and fn mean',\n",
    "'what is booking class c']\n",
    "\n",
    "for sent in sentences:\n",
    "\n",
    "   doc = nlp(sent)\n",
    "   matches = matcher(doc)\n",
    "   for mid, start, end in matches:\n",
    "     print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "def reach_parent(source_token, dest_token):\n",
    "\t\"\"\"Checks if source and desrination token are relarted\"\"\"\n",
    "\tsource_token = source_token.head\n",
    "\twhile source_token != dest_token:\n",
    "\t\tif source_token.head == source_token:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tsource_token = source_token.head\n",
    "\n",
    "\treturn source_token\n",
    "\n",
    "doc = nlp(\"I'm going to a conference in Munich.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source token:  Munich\n",
      "destination token:  to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "to"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"source token: \", doc[-2])\n",
    "print(\"destination token: \",doc[3])\n",
    "reach_parent(doc[-2], doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findFlight\n"
     ]
    }
   ],
   "source": [
    "# find direct object of a sentence. dobj.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"find a flight from washington to sf\")\n",
    "\n",
    "for token in doc:\n",
    "  if token.dep_ == \"dobj\":\n",
    "    print(token.head.text + token.text.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "912e2ba85d2ed857c5dad7d34e258ff6bfef2aa54144621c571ff6b56961e04e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('spacy-playground-YteZBk1j-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
